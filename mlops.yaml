# PIPELINE DEFINITION
# Name: my-pipeline
# Inputs:
#    access_key: str
#    branch_name: str
#    cloned_dir: str
#    dvc_file_dir: str
#    dvc_file_name: str
#    github_token: str
#    github_username: str
#    minio_url: str
#    remote_name: str
#    remote_url: str
#    repo_url: str
#    secret_key: str
components:
  comp-data-ingestion:
    executorLabel: exec-data-ingestion
    inputDefinitions:
      parameters:
        access_key:
          parameterType: STRING
        branch_name:
          parameterType: STRING
        cloned_dir:
          parameterType: STRING
        dvc_file_dir:
          parameterType: STRING
        dvc_file_name:
          parameterType: STRING
        github_token:
          parameterType: STRING
        github_username:
          parameterType: STRING
        minio_url:
          parameterType: STRING
        remote_name:
          parameterType: STRING
        remote_url:
          parameterType: STRING
        repo_url:
          parameterType: STRING
        secret_key:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-data-preparation:
    executorLabel: exec-data-preparation
    inputDefinitions:
      artifacts:
        dataset_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        random_state:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        test_size:
          defaultValue: 0.2
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        X_test_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        X_train_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_test_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_train_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-data-ingestion:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - data_ingestion
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'gitpython'\
          \ 'dvc==3.51.1' 'dvc-s3==3.2.0' 'numpy==1.25.2' 'pandas==2.0.3' && \"$0\"\
          \ \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef data_ingestion(\n    repo_url: str,\n    cloned_dir: str,\n \
          \   branch_name: str,\n    github_username: str,\n    github_token: str,\n\
          \    remote_name: str,\n    remote_url: str,\n    minio_url: str,\n    access_key:\
          \ str,\n    secret_key: str,\n    dvc_file_dir: str,\n    dvc_file_name:\
          \ str,\n    dataset_artifact: Output[Dataset]\n    ):\n    from git import\
          \ Repo\n    from subprocess import run, CalledProcessError\n    import os\n\
          \    import pandas as pd\n\n    def clone_repository_with_token(repo_url,\
          \ cloned_dir, branch_name, github_username, github_token):\n        \"\"\
          \"Clone a Git repository using a GitHub token in the URL and specifying\
          \ the branch.\"\"\"\n        try:\n            # Construct the URL with\
          \ the GitHub username and token\n            url_with_token = f\"https://{github_username}:{github_token}@{repo_url.split('//')[1]}\"\
          \n\n            # Clone the repository from the specified branch\n     \
          \       repo = Repo.clone_from(url_with_token, cloned_dir, branch=branch_name)\n\
          \            return \"Repository cloned successfully\"\n        except Exception\
          \ as e:\n            return f\"Error occurred during repository cloning:\
          \ {e}\"\n\n    def configure_dvc_remote(cloned_dir, remote_name, remote_url,\
          \ minio_url, access_key, secret_key):\n        \"\"\"Configure the Minio\
          \ bucket as the DVC remote repository using the `dvc remote` commands.\"\
          \"\"\n        try:\n            # Add the remote\n            run(\n   \
          \             ['dvc', 'remote', 'add', '-d', remote_name, remote_url],\n\
          \                cwd=cloned_dir,\n                capture_output=True,\n\
          \                text=True,\n                check=True\n            )\n\
          \n            # Configure the endpoint URL\n            run(\n         \
          \       ['dvc', 'remote', 'modify', remote_name, 'endpointurl', minio_url],\n\
          \                cwd=cloned_dir,\n                capture_output=True,\n\
          \                text=True,\n                check=True\n            )\n\
          \n            # Configure access key ID\n            run(\n            \
          \    ['dvc', 'remote', 'modify', remote_name, 'access_key_id', access_key],\n\
          \                cwd=cloned_dir,\n                capture_output=True,\n\
          \                text=True,\n                check=True\n            )\n\
          \n            # Configure secret access key\n            run(\n        \
          \        ['dvc', 'remote', 'modify', remote_name, 'secret_access_key', secret_key],\n\
          \                cwd=cloned_dir,\n                capture_output=True,\n\
          \                text=True,\n                check=True\n            )\n\
          \n            return f'Successfully configured Minio bucket as DVC remote\
          \ repository: {remote_name}'\n        except CalledProcessError as e:\n\
          \            # Log and raise any errors\n            return f'Failed to\
          \ configure DVC remote: {e.stderr}'\n\n    def perform_dvc_pull(cloned_dir,\
          \ remote_name):\n        \"\"\"Perform a DVC pull to synchronize local data\
          \ with the remote repository.\"\"\"\n        try:\n            # Run the\
          \ `dvc pull` command\n            result = run(['dvc', 'pull', '-r', remote_name],\
          \ cwd=cloned_dir, capture_output=True, text=True)\n\n            # Check\
          \ if the command executed successfully\n            if result.returncode\
          \ != 0:\n                # Log and raise an error if the command failed\n\
          \                error_message = f\"dvc pull failed with error: {result.stderr}\"\
          \n                raise Exception(error_message)\n\n            # Log successful\
          \ operation\n            return \"Successfully pulled data from remote DVC\
          \ repository\"\n\n        except Exception as e:\n            # Log and\
          \ handle the error\n            return f\"Error occurred during dvc pull:\
          \ {e}\"\n\n    # Call the functions\n    clone_result = clone_repository_with_token(repo_url,\
          \ cloned_dir, branch_name, github_username, github_token)\n    configure_result\
          \ = configure_dvc_remote(cloned_dir, remote_name, remote_url, minio_url,\
          \ access_key, secret_key)\n    dvc_pull_result = perform_dvc_pull(cloned_dir,\
          \ remote_name)\n\n    # Save dataset with pandas in Dataset artifact\n \
          \   pulled_dataset_path = os.path.join(cloned_dir, dvc_file_dir, dvc_file_name)\n\
          \    tmp_dataset_path = \"/tmp/\" + dvc_file_name\n    dataset = pd.read_csv(pulled_dataset_path)\n\
          \    dataset.to_pickle(tmp_dataset_path)\n    os.rename(tmp_dataset_path,\
          \ dataset_artifact.path)\n\n"
        image: python:3.11.9
    exec-data-preparation:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - data_preparation
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas==2.0.3'\
          \ 'numpy==1.25.2' 'torch==2.3.0' 'scikit-learn==1.2.2' 'imblearn' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef data_preparation(\n    dataset_artifact: Input[Dataset],\n  \
          \  X_train_artifact: Output[Dataset], \n    X_test_artifact: Output[Dataset],\n\
          \    y_train_artifact: Output[Dataset],\n    y_test_artifact: Output[Dataset],\n\
          \    test_size: float = 0.2, \n    random_state: int = 42\n    ):\n    import\
          \ pandas as pd\n    import numpy as np\n    from sklearn.model_selection\
          \ import train_test_split\n    from imblearn.over_sampling import SMOTE\n\
          \    from sklearn.preprocessing import StandardScaler\n    import torch\n\
          \    import os\n\n    # Load dataset from Dataset artifact\n    df = pd.read_pickle(dataset_artifact.path)\n\
          \n    # Handle null values and replace specific characters\n    #df = df.replace(['\
          \ ', '-',np.nan], 0) # There are null values\n    df = df.replace([' ',\
          \ '-', np.nan], np.nan)\n\n    # Selective columns for mean calculation\n\
          \    columns_to_convert = [\n        'CQI1', 'CQI2', 'CQI3', 'cSTD CQI',\
          \ 'cMajority', 'c25 P', 'c50 P', 'c75 P', \n        'RSRP1', 'RSRP2', 'RSRP3',\
          \ 'pMajority', 'p25 P', 'p50 P', 'p75 P', \n        'RSRQ1', 'RSRQ2', 'RSRQ3',\
          \ 'qMajority', 'q25 P', 'q50 P', 'q75 P', \n        'SNR1', 'SNR2', 'SNR3',\
          \ 'sMajority', 's25 P', 's50 P', 's75 P'\n    ]\n    df[columns_to_convert]\
          \ = df[columns_to_convert].astype(float)\n\n    # Replace np.nan with mean\
          \ values for selective columns\n    df[columns_to_convert] = df[columns_to_convert].fillna(df[columns_to_convert].mean())\n\
          \n    # Convert 'Stall' column to numerical values\n    df['Stall'].replace({'Yes':\
          \ 1, 'No': 0}, inplace=True)\n\n    X = df[columns_to_convert].values\n\
          \    y = df['Stall'].values\n\n    # Apply SMOTE for balancing the dataset\n\
          \    # oversample = SMOTE(random_state=random_state)\n    oversample = SMOTE()\n\
          \    X, y = oversample.fit_resample(X, y)\n\n    # Standardize the features\n\
          \    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n\n    #\
          \ Convert to torch tensors\n    X = torch.tensor(X, dtype=torch.float32)\n\
          \    y = torch.tensor(y, dtype=torch.float32)\n\n    # Split the dataset\
          \ into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X,\
          \ y, test_size=test_size, random_state=random_state)\n\n    X_train_path\
          \ = \"/tmp/X_train.pt\"\n    X_test_path = \"/tmp/X_test.pt\"\n    y_train_path\
          \ = \"/tmp/y_train.pt\"\n    y_test_path = \"/tmp/y_test.pt\"\n    torch.save(X_train,\
          \ X_train_path)\n    os.rename(X_train_path, X_train_artifact.path)\n\n\
          \    torch.save(X_test, X_test_path)\n    os.rename(X_test_path, X_test_artifact.path)\n\
          \n    torch.save(y_train, y_train_path)\n    os.rename(y_train_path, y_train_artifact.path)\n\
          \n    torch.save(y_test, y_test_path)\n    os.rename(y_test_path, y_test_artifact.path)\n\
          \n"
        image: python:3.11.9
pipelineInfo:
  name: my-pipeline
root:
  dag:
    tasks:
      data-ingestion:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-data-ingestion
        inputs:
          parameters:
            access_key:
              componentInputParameter: access_key
            branch_name:
              componentInputParameter: branch_name
            cloned_dir:
              componentInputParameter: cloned_dir
            dvc_file_dir:
              componentInputParameter: dvc_file_dir
            dvc_file_name:
              componentInputParameter: dvc_file_name
            github_token:
              componentInputParameter: github_token
            github_username:
              componentInputParameter: github_username
            minio_url:
              componentInputParameter: minio_url
            remote_name:
              componentInputParameter: remote_name
            remote_url:
              componentInputParameter: remote_url
            repo_url:
              componentInputParameter: repo_url
            secret_key:
              componentInputParameter: secret_key
        taskInfo:
          name: data-ingestion
      data-preparation:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-data-preparation
        dependentTasks:
        - data-ingestion
        inputs:
          artifacts:
            dataset_artifact:
              taskOutputArtifact:
                outputArtifactKey: dataset_artifact
                producerTask: data-ingestion
        taskInfo:
          name: data-preparation
  inputDefinitions:
    parameters:
      access_key:
        parameterType: STRING
      branch_name:
        parameterType: STRING
      cloned_dir:
        parameterType: STRING
      dvc_file_dir:
        parameterType: STRING
      dvc_file_name:
        parameterType: STRING
      github_token:
        parameterType: STRING
      github_username:
        parameterType: STRING
      minio_url:
        parameterType: STRING
      remote_name:
        parameterType: STRING
      remote_url:
        parameterType: STRING
      repo_url:
        parameterType: STRING
      secret_key:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
